model:
  model_config:
    hidden_size: 4096
    num_hidden_layers: 32
    vocab_size: 50432
    intermediate_size: 11008
    tie_word_embeddings: true
    num_attention_heads: 32
    num_key_value_heads: 32
parallelism:
  tp: 1
  pp: 1
  dp: 1
tokens:
  sequence_length: 2048
  micro_batch_size: 1
  batch_accumulation_per_replica: 1
optimizer:
  zero_stage: 0
  full_checkpointing: false
